{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KorQuAD 1.0 활용해서 Roberta-large fine-tuning 하기\n",
    "-> huggingface에 모델 올려두고 불러와서 사용하기! https://www.youtube.com/watch?v=ovD_87gHZO4\n",
    "\n",
    "- Method0. CNN layer 추가 \n",
    "- Method1. KorQuAD 1.0 (train+validation) 데이터셋 만으로 1차 fine-tuning -> ssunbear/klue_roberta_large_finetuned_korquad_v1\n",
    "- Method2. Method1에 mrc_train 데이터셋으로 한번 더 fine-tuning(모델 재호출) -> ssunbear/klue_roberta_large_finetuned_korquad_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: huggingface in /opt/conda/lib/python3.10/site-packages (0.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: datasets==2.14.6 in /opt/conda/lib/python3.10/site-packages (2.14.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6) (1.26.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6) (0.25.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.6) (6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.6) (4.0.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.6) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.6) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.6) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.6) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==2.14.6) (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.28.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.16.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.24.0 -q\n",
    "!pip install huggingface\n",
    "!pip install datasets==2.14.6\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## korquad 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "dataset = load_dataset('squad_kor_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 60407\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 5774\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mrc valid 데이터셋 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# 데이터셋 로드\n",
    "mrc_train_dataset_path = \"/data/ephemeral/home/level2-mrc-nlp-15/data/train_dataset/train\"  # 실제 데이터셋 경로로 수정\n",
    "mrc_train_dataset = load_from_disk(mrc_train_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
       "    num_rows: 3952\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrc_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# 데이터셋 로드\n",
    "mrc_validation_dataset_path = \"/data/ephemeral/home/level2-mrc-nlp-15/data/train_dataset/validation\"  # 실제 데이터셋 경로로 수정\n",
    "mrc_validation_dataset = load_from_disk(mrc_validation_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
       "    num_rows: 240\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrc_validation_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# 데이터셋 로드\n",
    "mrc_train_validation_path = \"/data/ephemeral/home/level2-mrc-nlp-15/data/train_dataset/validation\"  # 실제 데이터셋 경로로 수정\n",
    "mrc_train_validation = load_from_disk(mrc_train_validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# korquad 데이터셋이랑 형식 똑같이 만들어주기\n",
    "id_list = []\n",
    "title_list = []\n",
    "context_list = []\n",
    "question_list = []\n",
    "answers_list = []\n",
    "\n",
    "for index, row in pd.DataFrame(mrc_train_dataset).iterrows():\n",
    "    id_list.append(row['id'])\n",
    "    title_list.append(str(row['title']))\n",
    "    context_list.append(str(row['context']))\n",
    "    question_list.append(str(row['question']))\n",
    "    answers_list.append(row['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrc_train_dataset = {\n",
    "    \"id\" : id_list,\n",
    "    \"title\" : title_list,\n",
    "    \"context\" : context_list,\n",
    "    \"question\" : question_list,\n",
    "    \"answers\" : answers_list,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 3\u001b[0m mrc_train_dataset\u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmrc_train_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m mrc_train_dataset\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:900\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    898\u001b[0m features \u001b[38;5;241m=\u001b[39m features \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m info\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    899\u001b[0m arrow_typed_mapping \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 900\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)):\n\u001b[1;32m    902\u001b[0m         data \u001b[38;5;241m=\u001b[39m cast_array_to_feature(data, features[col]) \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m data\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "mrc_train_dataset= Dataset.from_dict(mrc_train_dataset)\n",
    "mrc_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# korquad 데이터셋이랑 형식 똑같이 만들어주기\n",
    "id_list2 = []\n",
    "title_list2 = []\n",
    "context_list2 = []\n",
    "question_list2 = []\n",
    "answers_list2 = []\n",
    "\n",
    "for index, row in pd.DataFrame(mrc_validation_dataset).iterrows():\n",
    "    id_list2.append(row['id'])\n",
    "    title_list2.append(str(row['title']))\n",
    "    context_list2.append(str(row['context']))\n",
    "    question_list2.append(str(row['question']))\n",
    "    answers_list2.append(row['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrc_validation_dataset = {\n",
    "    \"id\" : id_list,\n",
    "    \"title\" : title_list,\n",
    "    \"context\" : context_list,\n",
    "    \"question\" : question_list,\n",
    "    \"answers\" : answers_list,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 3\u001b[0m mrc_validation_dataset\u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmrc_validation_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m mrc_validation_dataset\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:900\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    898\u001b[0m features \u001b[38;5;241m=\u001b[39m features \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m info\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    899\u001b[0m arrow_typed_mapping \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 900\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)):\n\u001b[1;32m    902\u001b[0m         data \u001b[38;5;241m=\u001b[39m cast_array_to_feature(data, features[col]) \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m data\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "mrc_validation_dataset= Dataset.from_dict(mrc_validation_dataset)\n",
    "mrc_validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## korquad 데이터셋 filtering\n",
    "- Korquad 데이터셋과 train 데이터셋의 context 길이 분포 맞춰주기\n",
    "- train 데이터셋 context 길이 2064 이하이므로, korquad 데이터셋 중 context 길이가 2064개 이상인 데이터들은 삭제해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = dataset['train'].filter(lambda example: len(example['context']) <= 2064)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset_validation = dataset['validation'].filter(lambda example: len(example['context']) <= 2064)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list3 = []\n",
    "title_list3 = []\n",
    "context_list3 = []\n",
    "question_list3 = []\n",
    "answers_list3 = []\n",
    "\n",
    "for index, row in pd.DataFrame(filtered_dataset_validation).iterrows():\n",
    "    id_list3.append(row['id'])\n",
    "    title_list3.append(str(row['title']))\n",
    "    context_list3.append(str(row['context']))\n",
    "    question_list3.append(str(row['question']))\n",
    "    answers_list3.append(row['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = {\n",
    "    \"id\" : id_list3,\n",
    "    \"title\" : title_list3,\n",
    "    \"context\" : context_list3,\n",
    "    \"question\" : question_list3,\n",
    "    \"answers\" : answers_list3,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 5735\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val_dataset= Dataset.from_dict(val_dataset)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained 모델 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing CNN_RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CNN_RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CNN_RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CNN_RobertaForQuestionAnswering were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['cnn_block2.conv1.weight', 'cnn_block1.layer_norm.weight', 'cnn_block1.conv2.bias', 'cnn_block5.conv1.bias', 'cnn_block3.conv1.bias', 'cnn_block5.conv2.bias', 'cnn_block4.conv1.bias', 'cnn_block2.conv2.bias', 'cnn_block5.conv1.weight', 'cnn_block2.layer_norm.weight', 'cnn_block3.conv1.weight', 'cnn_block5.layer_norm.weight', 'cnn_block4.conv1.weight', 'cnn_block1.conv1.weight', 'cnn_block3.layer_norm.bias', 'cnn_block4.conv2.bias', 'qa_outputs.bias', 'qa_outputs.weight', 'cnn_block2.conv1.bias', 'cnn_block2.conv2.weight', 'cnn_block1.conv1.bias', 'cnn_block5.conv2.weight', 'cnn_block3.conv2.weight', 'cnn_block4.layer_norm.weight', 'cnn_block4.layer_norm.bias', 'cnn_block5.layer_norm.bias', 'cnn_block2.layer_norm.bias', 'cnn_block1.layer_norm.bias', 'cnn_block1.conv2.weight', 'cnn_block3.conv2.bias', 'cnn_block4.conv2.weight', 'cnn_block3.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    #AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from CNN_layer_model import CNN_RobertaForQuestionAnswering\n",
    "\n",
    "model_name = \"klue/roberta-large\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True\n",
    ")\n",
    "model = CNN_RobertaForQuestionAnswering.from_pretrained(\n",
    "    model_name,\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Korquad 데이터셋 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512 # 질문과 컨텍스트, special token을 합한 문자열의 최대 길이 (일정 개수가 넘어가지 않도록!)\n",
    "pad_to_max_length = False\n",
    "doc_stride = 128 # 컨텍스트가 너무 길어서 나눴을 때 오버랩되는 시퀀스 길이, 문서 2개로 쪼개고, 128개 시퀀스가 겹치도록\n",
    "preprocessing_num_workers = None\n",
    "batch_size = 16\n",
    "num_train_epochs = 1\n",
    "n_best_size = 20\n",
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples): # examples: 데이터셋 row..\n",
    "    # 주어진 텍스트를 토크나이징 한다. 이 때 텍스트의 길이가 max_seq_length를 넘으면 stride만큼 슬라이딩하며 여러 개로 쪼갬.\n",
    "    # 즉, 하나의 example에서 일부분이 겹치는 여러 sequence(feature)가 생길 수 있음.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",  # max_seq_length까지 truncate한다. pair의 두번째 파트(context)만 잘라냄.\n",
    "        max_length=max_seq_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True, # 길이를 넘어가는 토큰들을 반환할 것인지\n",
    "        return_offsets_mapping=True,  # 각 토큰에 대해 (char_start, char_end) 정보를 반환한 것인지\n",
    "        padding=\"max_length\", return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    # example 하나가 여러 sequence에 대응하는 경우를 위해 매핑이 필요함.\n",
    "    overflow_to_sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # offset_mappings으로 토큰이 원본 context 내 몇번째 글자부터 몇번째 글자까지 해당하는지 알 수 있음.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 정답지를 만들기 위한 리스트\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # 해당 example에 해당하는 sequence를 찾음.\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # sequence가 속하는 example을 찾는다.\n",
    "        example_index = overflow_to_sample_mapping[i]\n",
    "        answers = examples[\"answers\"][example_index]\n",
    "\n",
    "        # 텍스트에서 answer의 시작점, 끝점\n",
    "        answer_start_offset = answers[\"answer_start\"][0]\n",
    "        answer_end_offset = answer_start_offset + len(answers[\"text\"][0])\n",
    "\n",
    "        # 텍스트에서 현재 span의 시작 토큰 인덱스\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        # 텍스트에서 현재 span 끝 토큰 인덱스\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        # answer가 현재 span을 벗어났는지 체크\n",
    "        if not (offsets[token_start_index][0] <= answer_start_offset and offsets[token_end_index][1] >= answer_end_offset):\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # token_start_index와 token_end_index를 answer의 시작점과 끝점으로 옮김\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start_offset:\n",
    "                token_start_index += 1\n",
    "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "            while offsets[token_end_index][1] >= answer_end_offset:\n",
    "                token_end_index -= 1\n",
    "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = filtered_dataset.column_names\n",
    "train_dataset = filtered_dataset.map(\n",
    "            prepare_train_features,\n",
    "            batched=True,\n",
    "            num_proc=preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples['question'],\n",
    "        examples['context'],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5735/5735 [00:04<00:00, 1429.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = val_dataset.map(\n",
    "            prepare_validation_features,\n",
    "            batched=True,\n",
    "            num_proc=preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering Class 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_data_collator: 여러개 example들을 collator해주는 역할,\n",
    "# TrainingArguments : 한번에 training arguments들을 합쳐서 주는..!\n",
    "from transformers import default_data_collator, TrainingArguments, EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Team All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Question-Answering task와 관련된 'Trainer'의 subclass 코드 입니다.\n",
    "\"\"\"\n",
    "\n",
    "from transformers import Trainer, is_datasets_available, is_torch_tpu_available\n",
    "from transformers.trainer_utils import PredictionOutput\n",
    "\n",
    "if is_datasets_available():\n",
    "    import datasets\n",
    "\n",
    "# Huggingface의 Trainer를 상속받아 QuestionAnswering을 위한 Trainer를 생성합니다.\n",
    "class QuestionAnsweringTrainer(Trainer):\n",
    "    def __init__(self, *args, eval_examples=None, post_process_function=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.eval_examples = eval_examples\n",
    "        self.post_process_function = post_process_function\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None):\n",
    "        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        eval_examples = self.eval_examples if eval_examples is None else eval_examples\n",
    "\n",
    "        # 일시적으로 metric computation를 불가능하게 한 상태이며, 해당 코드에서는 loop 내에서 metric 계산을 수행합니다.\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        try:\n",
    "            output = self.prediction_loop(\n",
    "                eval_dataloader,\n",
    "                description=\"Evaluation\",\n",
    "                # metric이 없으면 예측값을 모으는 이유가 없으므로 아래의 코드를 따르게 됩니다.\n",
    "                # self.args.prediction_loss_only\n",
    "                prediction_loss_only=True if compute_metrics is None else None,\n",
    "                ignore_keys=ignore_keys,\n",
    "            )\n",
    "        finally:\n",
    "            self.compute_metrics = compute_metrics\n",
    "\n",
    "        if isinstance(eval_dataset, datasets.Dataset):\n",
    "            eval_dataset.set_format(\n",
    "                type=eval_dataset.format[\"type\"],\n",
    "                columns=list(eval_dataset.features.keys()),\n",
    "            )\n",
    "\n",
    "        if self.post_process_function is not None and self.compute_metrics is not None:\n",
    "            eval_preds = self.post_process_function(\n",
    "                eval_examples, eval_dataset, output.predictions, self.args\n",
    "            )\n",
    "            metrics = self.compute_metrics(eval_preds)\n",
    "\n",
    "            self.log(metrics)\n",
    "        else:\n",
    "            metrics = {}\n",
    "\n",
    "        self.control = self.callback_handler.on_evaluate(\n",
    "            self.args, self.state, self.control, metrics\n",
    "        )\n",
    "        return metrics\n",
    "\n",
    "    def predict(self, test_dataset, test_examples, ignore_keys=None):\n",
    "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
    "\n",
    "        # 일시적으로 metric computation를 불가능하게 한 상태이며, 해당 코드에서는 loop 내에서 metric 계산을 수행합니다.\n",
    "        # evaluate 함수와 동일하게 구성되어있습니다\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        try:\n",
    "            output = self.prediction_loop(\n",
    "                test_dataloader,\n",
    "                description=\"Evaluation\",\n",
    "                # metric이 없으면 예측값을 모으는 이유가 없으므로 아래의 코드를 따르게 됩니다.\n",
    "                # self.args.prediction_loss_only\n",
    "                prediction_loss_only=True if compute_metrics is None else None,\n",
    "                ignore_keys=ignore_keys,\n",
    "            )\n",
    "        finally:\n",
    "            self.compute_metrics = compute_metrics\n",
    "\n",
    "        if self.post_process_function is None or self.compute_metrics is None:\n",
    "            return output\n",
    "\n",
    "        if isinstance(test_dataset, datasets.Dataset):\n",
    "            test_dataset.set_format(\n",
    "                type=test_dataset.format[\"type\"],\n",
    "                columns=list(test_dataset.features.keys()),\n",
    "            )\n",
    "\n",
    "        predictions = self.post_process_function(\n",
    "            test_examples, test_dataset, output.predictions, self.args\n",
    "        )\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 후처리 클래스 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Team All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the 'License');\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an 'AS IS' BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Pre-processing\n",
    "Post-processing utilities for question answering.\n",
    "\"\"\"\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from arguments import DataTrainingArguments, ModelArguments\n",
    "from datasets import DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PreTrainedTokenizerFast, TrainingArguments, is_torch_available\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "#from utils.datetime_helper import get_seoul_datetime_str\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 2024):\n",
    "    \"\"\"\n",
    "    seed 고정하는 함수 (random, numpy, torch)\n",
    "\n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if is_torch_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def postprocess_qa_predictions(\n",
    "    examples,\n",
    "    features,\n",
    "    predictions: Tuple[np.ndarray, np.ndarray],\n",
    "    version_2_with_negative: bool = False,\n",
    "    n_best_size: int = 20,\n",
    "    max_answer_length: int = 30,\n",
    "    null_score_diff_threshold: float = 0.0,\n",
    "    output_dir: Optional[str] = None,\n",
    "    prefix: Optional[str] = None,\n",
    "    is_world_process_zero: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Post-processes : qa model의 prediction 값을 후처리하는 함수\n",
    "    모델은 start logit과 end logit을 반환하기 때문에, 이를 기반으로 original text로 변경하는 후처리가 필요함\n",
    "\n",
    "    Args:\n",
    "        examples: 전처리 되지 않은 데이터셋 (see the main script for more information).\n",
    "        features: 전처리가 진행된 데이터셋 (see the main script for more information).\n",
    "        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\n",
    "            모델의 예측값 :start logits과 the end logits을 나타내는 two arrays              첫번째 차원은 :obj:`features`의 element와 갯수가 맞아야함.\n",
    "        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            정답이 없는 데이터셋이 포함되어있는지 여부를 나타냄\n",
    "        n_best_size (:obj:`int`, `optional`, defaults to 20):\n",
    "            답변을 찾을 때 생성할 n-best prediction 총 개수\n",
    "        max_answer_length (:obj:`int`, `optional`, defaults to 30):\n",
    "            생성할 수 있는 답변의 최대 길이\n",
    "        null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):\n",
    "            null 답변을 선택하는 데 사용되는 threshold\n",
    "            : if the best answer has a score that is less than the score of\n",
    "            the null answer minus this threshold, the null answer is selected for this example (note that the score of\n",
    "            the null answer for an example giving several features is the minimum of the scores for the null answer on\n",
    "            each feature: all features must be aligned on the fact they `want` to predict a null answer).\n",
    "            Only useful when :obj:`version_2_with_negative` is :obj:`True`.\n",
    "        output_dir (:obj:`str`, `optional`):\n",
    "            아래의 값이 저장되는 경로\n",
    "            dictionary : predictions, n_best predictions (with their scores and logits) if:obj:`version_2_with_negative=True`,\n",
    "            dictionary : the scores differences between best and null answers\n",
    "        prefix (:obj:`str`, `optional`):\n",
    "            dictionary에 `prefix`가 포함되어 저장됨\n",
    "        is_world_process_zero (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            이 프로세스가 main process인지 여부(logging/save를 수행해야 하는지 여부를 결정하는 데 사용됨)\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        len(predictions) == 2\n",
    "    ), \"`predictions` should be a tuple with two elements (start_logits, end_logits).\"\n",
    "    all_start_logits, all_end_logits = predictions\n",
    "\n",
    "    assert len(predictions[0]) == len(\n",
    "        features\n",
    "    ), f\"Got {len(predictions[0])} predictions and {len(features)} features.\"\n",
    "\n",
    "    # example과 mapping되는 feature 생성\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # prediction, nbest에 해당하는 OrderedDict 생성합니다.\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "    if version_2_with_negative:\n",
    "        scores_diff_json = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    logger.setLevel(logging.INFO if is_world_process_zero else logging.WARN)\n",
    "    logger.info(\n",
    "        f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\"\n",
    "    )\n",
    "\n",
    "    # 전체 example들에 대한 main Loop\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # 해당하는 현재 example index\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_prediction = None\n",
    "        prelim_predictions = []\n",
    "\n",
    "        # 현재 example에 대한 모든 feature 생성합니다.\n",
    "        for feature_index in feature_indices:\n",
    "            # 각 featureure에 대한 모든 prediction을 가져옵니다.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # logit과 original context의 logit을 mapping합니다.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "            # Optional : `token_is_max_context`, 제공되는 경우 현재 기능에서 사용할 수 있는 max context가 없는 answer를 제거합니다\n",
    "            token_is_max_context = features[feature_index].get(\n",
    "                \"token_is_max_context\", None\n",
    "            )\n",
    "\n",
    "            # minimum null prediction을 업데이트 합니다.\n",
    "            feature_null_score = start_logits[0] + end_logits[0]\n",
    "            if (\n",
    "                min_null_prediction is None\n",
    "                or min_null_prediction[\"score\"] > feature_null_score\n",
    "            ):\n",
    "                min_null_prediction = {\n",
    "                    \"offsets\": (0, 0),\n",
    "                    \"score\": feature_null_score,\n",
    "                    \"start_logit\": start_logits[0],\n",
    "                    \"end_logit\": end_logits[0],\n",
    "                }\n",
    "\n",
    "            # `n_best_size`보다 큰 start and end logits을 살펴봅니다.\n",
    "            start_indexes = np.argsort(start_logits)[\n",
    "                -1 : -n_best_size - 1 : -1\n",
    "            ].tolist()\n",
    "\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # out-of-scope answers는 고려하지 않습니다.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # 길이가 < 0 또는 > max_answer_length인 answer도 고려하지 않습니다.\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "                    # 최대 context가 없는 answer도 고려하지 않습니다.\n",
    "                    if (\n",
    "                        token_is_max_context is not None\n",
    "                        and not token_is_max_context.get(str(start_index), False)\n",
    "                    ):\n",
    "                        continue\n",
    "                    prelim_predictions.append(\n",
    "                        {\n",
    "                            \"offsets\": (\n",
    "                                offset_mapping[start_index][0],\n",
    "                                offset_mapping[end_index][1],\n",
    "                            ),\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"start_logit\": start_logits[start_index],\n",
    "                            \"end_logit\": end_logits[end_index],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if version_2_with_negative:\n",
    "            # minimum null prediction을 추가합니다.\n",
    "            prelim_predictions.append(min_null_prediction)\n",
    "            null_score = min_null_prediction[\"score\"]\n",
    "\n",
    "        # 가장 좋은 `n_best_size` predictions만 유지합니다.\n",
    "        predictions = sorted(\n",
    "            prelim_predictions, key=lambda x: x[\"score\"], reverse=True\n",
    "        )[:n_best_size]\n",
    "\n",
    "        # 낮은 점수로 인해 제거된 경우 minimum null prediction을 다시 추가합니다.\n",
    "        if version_2_with_negative and not any(\n",
    "            p[\"offsets\"] == (0, 0) for p in predictions\n",
    "        ):\n",
    "            predictions.append(min_null_prediction)\n",
    "\n",
    "        # offset을 사용하여 original context에서 answer text를 수집합니다.\n",
    "        context = example[\"context\"]\n",
    "        for pred in predictions:\n",
    "            offsets = pred.pop(\"offsets\")\n",
    "            pred[\"text\"] = context[offsets[0] : offsets[1]]\n",
    "\n",
    "        # rare edge case에는 null이 아닌 예측이 하나도 없으며 failure를 피하기 위해 fake prediction을 만듭니다.\n",
    "        if len(predictions) == 0 or (\n",
    "            len(predictions) == 1 and predictions[0][\"text\"] == \"\"\n",
    "        ):\n",
    "\n",
    "            predictions.insert(\n",
    "                0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0}\n",
    "            )\n",
    "\n",
    "        # 모든 점수의 소프트맥스를 계산합니다(we do it with numpy to stay independent from torch/tf in this file, using the LogSumExp trick).\n",
    "        scores = np.array([pred.pop(\"score\") for pred in predictions])\n",
    "        exp_scores = np.exp(scores - np.max(scores))\n",
    "        probs = exp_scores / exp_scores.sum()\n",
    "\n",
    "        # 예측값에 확률을 포함합니다.\n",
    "        for prob, pred in zip(probs, predictions):\n",
    "            pred[\"probability\"] = prob\n",
    "\n",
    "        # best prediction을 선택합니다.\n",
    "        if not version_2_with_negative:\n",
    "            all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n",
    "        else:\n",
    "            # else case : 먼저 비어 있지 않은 최상의 예측을 찾아야 합니다\n",
    "            i = 0\n",
    "            while predictions[i][\"text\"] == \"\":\n",
    "                i += 1\n",
    "            best_non_null_pred = predictions[i]\n",
    "\n",
    "            # threshold를 사용해서 null prediction을 비교합니다.\n",
    "            score_diff = (\n",
    "                null_score\n",
    "                - best_non_null_pred[\"start_logit\"]\n",
    "                - best_non_null_pred[\"end_logit\"]\n",
    "            )\n",
    "            scores_diff_json[example[\"id\"]] = float(score_diff)  # JSON-serializable 가능\n",
    "            if score_diff > null_score_diff_threshold:\n",
    "                all_predictions[example[\"id\"]] = \"\"\n",
    "            else:\n",
    "                all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n",
    "\n",
    "        # np.float를 다시 float로 casting -> `predictions`은 JSON-serializable 가능\n",
    "        all_nbest_json[example[\"id\"]] = [\n",
    "            {\n",
    "                k: (\n",
    "                    float(v)\n",
    "                    if isinstance(v, (np.float16, np.float32, np.float64))\n",
    "                    else v\n",
    "                )\n",
    "                for k, v in pred.items()\n",
    "            }\n",
    "            for pred in predictions\n",
    "        ]\n",
    "\n",
    "    # output_dir이 있으면 모든 dicts를 저장합니다.\n",
    "    if output_dir is not None:\n",
    "        assert os.path.isdir(output_dir), f\"{output_dir} is not a directory.\"\n",
    "\n",
    "        prediction_file = os.path.join(\n",
    "            output_dir,\n",
    "            \"predictions.json\" if prefix is None else f\"predictions_{prefix}.json\",\n",
    "        )\n",
    "        nbest_file = os.path.join(\n",
    "            output_dir,\n",
    "            \"nbest_predictions.json\"\n",
    "            if prefix is None\n",
    "            else f\"nbest_predictions_{prefix}.json\",\n",
    "        )\n",
    "        if version_2_with_negative:\n",
    "            null_odds_file = os.path.join(\n",
    "                output_dir,\n",
    "                \"null_odds.json\" if prefix is None else f\"null_odds_{prefix}.json\",\n",
    "            )\n",
    "\n",
    "        logger.info(f\"Saving predictions to {prediction_file}.\")\n",
    "        with open(prediction_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            writer.write(\n",
    "                json.dumps(all_predictions, indent=4, ensure_ascii=False) + \"\\n\"\n",
    "            )\n",
    "        logger.info(f\"Saving nbest_preds to {nbest_file}.\")\n",
    "        with open(nbest_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            writer.write(\n",
    "                json.dumps(all_nbest_json, indent=4, ensure_ascii=False) + \"\\n\"\n",
    "            )\n",
    "        if version_2_with_negative:\n",
    "            logger.info(f\"Saving null_odds to {null_odds_file}.\")\n",
    "            with open(null_odds_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "                writer.write(\n",
    "                    json.dumps(scores_diff_json, indent=4, ensure_ascii=False) + \"\\n\"\n",
    "                )\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "def check_no_error(\n",
    "    data_args: DataTrainingArguments,\n",
    "    training_args: TrainingArguments,\n",
    "    datasets: DatasetDict,\n",
    "    tokenizer,\n",
    ") -> Tuple[Any, int]:\n",
    "\n",
    "    # last checkpoint 찾기.\n",
    "    last_checkpoint = None\n",
    "    if (\n",
    "        os.path.isdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "    # Tokenizer check: 해당 script는 Fast tokenizer를 필요로합니다.\n",
    "    if not isinstance(tokenizer, PreTrainedTokenizerFast):\n",
    "        raise ValueError(\n",
    "            \"This example script only works for models that have a fast tokenizer. Checkout the big table of models \"\n",
    "            \"at https://huggingface.co/transformers/index.html#bigtable to find the model types that meet this \"\n",
    "            \"requirement\"\n",
    "        )\n",
    "\n",
    "    if data_args.max_seq_length > tokenizer.model_max_length:\n",
    "        logger.warn(\n",
    "            f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "    if \"validation\" not in datasets:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    return last_checkpoint, max_seq_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 후처리 함수 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 이해하는 형태에서 사람이 이해하는 형태로 답변 매칭\n",
    "def post_processing_function(examples, features, predictions):\n",
    "    # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "    predictions = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        version_2_with_negative=False,\n",
    "        n_best_size=n_best_size,\n",
    "        max_answer_length=max_answer_length,\n",
    "        null_score_diff_threshold=0.0,\n",
    "        output_dir=training_args.output_dir,\n",
    "        is_world_process_zero=trainer.is_world_process_zero(),\n",
    "    )\n",
    "\n",
    "    # Format the result to the format the metric expects.\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "    return EvalPrediction(predictions=formatted_predictions, label_ids=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/level2-mrc-nlp-15/src/wandb/run-20241017_040517-ua6lvn4p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp15/odqa/runs/ua6lvn4p' target=\"_blank\">dandy-rain-976</a></strong> to <a href='https://wandb.ai/nlp15/odqa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp15/odqa' target=\"_blank\">https://wandb.ai/nlp15/odqa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp15/odqa/runs/ua6lvn4p' target=\"_blank\">https://wandb.ai/nlp15/odqa/runs/ua6lvn4p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp15/odqa/runs/ua6lvn4p?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc036111e40>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project='odqa', # 실험기록을 관리한 프로젝트 이름\n",
    "           entity='nlp15', # 사용자명 또는 팀 이름        \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_total_limit=1,  # 저장할 체크포인트의 최대 수\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    #eval_steps=500,  # 몇 스텝마다 평가할지 설정\n",
    "    #logging_steps=500,  # 몇 스텝마다 로깅할지 설정,\n",
    "    #load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        eval_examples=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator, # 보통 default\n",
    "        post_process_function=post_processing_function, # function을 input으로 받음!\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 62641\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3916\n",
      "  Number of trainable parameters = 356600834\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3916' max='3916' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3916/3916 1:43:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.769300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.350300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs/checkpoint-500\n",
      "Configuration saved in outputs/checkpoint-500/config.json\n",
      "Model weights saved in outputs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to outputs/checkpoint-1000\n",
      "Configuration saved in outputs/checkpoint-1000/config.json\n",
      "Model weights saved in outputs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to outputs/checkpoint-1500\n",
      "Configuration saved in outputs/checkpoint-1500/config.json\n",
      "Model weights saved in outputs/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to outputs/checkpoint-2000\n",
      "Configuration saved in outputs/checkpoint-2000/config.json\n",
      "Model weights saved in outputs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to outputs/checkpoint-2500\n",
      "Configuration saved in outputs/checkpoint-2500/config.json\n",
      "Model weights saved in outputs/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to outputs/checkpoint-3000\n",
      "Configuration saved in outputs/checkpoint-3000/config.json\n",
      "Model weights saved in outputs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to outputs/checkpoint-3500\n",
      "Configuration saved in outputs/checkpoint-3500/config.json\n",
      "Model weights saved in outputs/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-3000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>train/learning_rate</td><td>█▇▆▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▄▃▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>3916</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.3503</td></tr><tr><td>train/total_flos</td><td>6.221469142067405e+16</td></tr><tr><td>train/train_loss</td><td>0.47893</td></tr><tr><td>train/train_runtime</td><td>6235.4731</td></tr><tr><td>train/train_samples_per_second</td><td>10.046</td></tr><tr><td>train/train_steps_per_second</td><td>0.628</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dandy-rain-976</strong> at: <a href='https://wandb.ai/nlp15/odqa/runs/ua6lvn4p' target=\"_blank\">https://wandb.ai/nlp15/odqa/runs/ua6lvn4p</a><br/> View project at: <a href='https://wandb.ai/nlp15/odqa' target=\"_blank\">https://wandb.ai/nlp15/odqa</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241017_040517-ua6lvn4p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cnn_block1): CNN_block(\n",
       "    (conv1): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (cnn_block2): CNN_block(\n",
       "    (conv1): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (cnn_block3): CNN_block(\n",
       "    (conv1): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (cnn_block4): CNN_block(\n",
       "    (conv1): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (cnn_block5): CNN_block(\n",
       "    (conv1): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 허깅페이스에 모델 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: sudo: command not found\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/tmp_r5qc71i/config.json\n",
      "Model weights saved in /tmp/tmp_r5qc71i/pytorch_model.bin\n",
      "Uploading the following files to ssunbear/klue_roberta_large_finetuned_korquad_v1: pytorch_model.bin,config.json\n",
      "pytorch_model.bin: 100%|██████████| 1.43G/1.43G [00:48<00:00, 29.1MB/s]   \n",
      "tokenizer config file saved in /tmp/tmpxrg7mu12/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpxrg7mu12/special_tokens_map.json\n",
      "Uploading the following files to ssunbear/klue_roberta_large_finetuned_korquad_v1: tokenizer_config.json,vocab.txt,tokenizer.json,special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ssunbear/klue_roberta_large_finetuned_korquad_v1/commit/bd3c260ee793ce46ab444055515bf02be74f2a80', commit_message='Upload tokenizer', commit_description='', oid='bd3c260ee793ce46ab444055515bf02be74f2a80', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ssunbear/klue_roberta_large_finetuned_korquad_v1', endpoint='https://huggingface.co', repo_type='model', repo_id='ssunbear/klue_roberta_large_finetuned_korquad_v1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Huggingface Access Token\n",
    "ACCESS_TOKEN = # 토큰아이디 입력하시면 됩니다\n",
    "\n",
    "# Upload to Huggingface\n",
    "model.push_to_hub('klue_roberta_large_finetuned_korquad_v1', use_temp_dir=True, use_auth_token=ACCESS_TOKEN)\n",
    "tokenizer.push_to_hub('klue_roberta_large_finetuned_korquad_v1', use_temp_dir=True, use_auth_token=ACCESS_TOKEN)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Method2. Method1에 mrc_train 데이터셋으로 한번 더 fine-tuning(모델 재호출) -> ssunbear/klue_roberta_large_finetuned_korquad_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--ssunbear--klue_roberta_large_finetuned_korquad_v1/snapshots/0ebea4e740f7702b26666664e61deaca4f7cb0dc/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ssunbear/klue_roberta_large_finetuned_korquad_v1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /data/ephemeral/home/.cache/huggingface/hub/models--ssunbear--klue_roberta_large_finetuned_korquad_v1/snapshots/0ebea4e740f7702b26666664e61deaca4f7cb0dc/vocab.txt\n",
      "loading file tokenizer.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--ssunbear--klue_roberta_large_finetuned_korquad_v1/snapshots/0ebea4e740f7702b26666664e61deaca4f7cb0dc/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--ssunbear--klue_roberta_large_finetuned_korquad_v1/snapshots/0ebea4e740f7702b26666664e61deaca4f7cb0dc/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--ssunbear--klue_roberta_large_finetuned_korquad_v1/snapshots/0ebea4e740f7702b26666664e61deaca4f7cb0dc/tokenizer_config.json\n",
      "loading weights file pytorch_model.bin from cache at /data/ephemeral/home/.cache/huggingface/hub/models--ssunbear--klue_roberta_large_finetuned_korquad_v1/snapshots/0ebea4e740f7702b26666664e61deaca4f7cb0dc/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at ssunbear/klue_roberta_large_finetuned_korquad_v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "model_name = \"ssunbear/klue_roberta_large_finetuned_korquad_v1\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True\n",
    ")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    model_name,\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## Train 데이터셋 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512 # 질문과 컨텍스트, special token을 합한 문자열의 최대 길이 (일정 개수가 넘어가지 않도록!)\n",
    "pad_to_max_length = False\n",
    "doc_stride = 128 # 컨텍스트가 너무 길어서 나눴을 때 오버랩되는 시퀀스 길이, 문서 2개로 쪼개고, 128개 시퀀스가 겹치도록\n",
    "preprocessing_num_workers = None\n",
    "batch_size = 16\n",
    "num_train_epochs = 4\n",
    "n_best_size = 20\n",
    "max_answer_length = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples): # examples: 데이터셋 row..\n",
    "    # 주어진 텍스트를 토크나이징 한다. 이 때 텍스트의 길이가 max_seq_length를 넘으면 stride만큼 슬라이딩하며 여러 개로 쪼갬.\n",
    "    # 즉, 하나의 example에서 일부분이 겹치는 여러 sequence(feature)가 생길 수 있음.\n",
    "    \n",
    "    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
    "\n",
    "    # Padding에 대한 옵션을 설정합니다.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",  # max_seq_length까지 truncate한다. pair의 두번째 파트(context)만 잘라냄.\n",
    "        max_length=max_seq_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True, # 길이를 넘어가는 토큰들을 반환할 것인지\n",
    "        return_offsets_mapping=True,  # 각 토큰에 대해 (char_start, char_end) 정보를 반환한 것인지\n",
    "        padding=\"max_length\", return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    # example 하나가 여러 sequence에 대응하는 경우를 위해 매핑이 필요함.\n",
    "    overflow_to_sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # offset_mappings으로 토큰이 원본 context 내 몇번째 글자부터 몇번째 글자까지 해당하는지 알 수 있음.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 정답지를 만들기 위한 리스트\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # 해당 example에 해당하는 sequence를 찾음.\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # sequence가 속하는 example을 찾는다.\n",
    "        example_index = overflow_to_sample_mapping[i]\n",
    "        answers = examples[\"answers\"][example_index]\n",
    "\n",
    "        # 텍스트에서 answer의 시작점, 끝점\n",
    "        answer_start_offset = answers[\"answer_start\"][0]\n",
    "        answer_end_offset = answer_start_offset + len(answers[\"text\"][0])\n",
    "\n",
    "        # 텍스트에서 현재 span의 시작 토큰 인덱스\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        # 텍스트에서 현재 span 끝 토큰 인덱스\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        # answer가 현재 span을 벗어났는지 체크\n",
    "        if not (offsets[token_start_index][0] <= answer_start_offset and offsets[token_end_index][1] >= answer_end_offset):\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # token_start_index와 token_end_index를 answer의 시작점과 끝점으로 옮김\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start_offset:\n",
    "                token_start_index += 1\n",
    "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "            while offsets[token_end_index][1] >= answer_end_offset:\n",
    "                token_end_index -= 1\n",
    "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3952/3952 [00:02<00:00, 1426.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "column_names = mrc_train_dataset.column_names\n",
    "train_dataset = mrc_train_dataset.map(\n",
    "            prepare_train_features,\n",
    "            batched=True,\n",
    "            num_proc=preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
       "    num_rows: 240\n",
       "})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# 데이터셋 로드\n",
    "mrc_validation_dataset_path = \"/data/ephemeral/home/level2-mrc-nlp-15/data/train_dataset/validation\"  # 실제 데이터셋 경로로 수정\n",
    "mrc_validation_dataset = load_from_disk(mrc_validation_dataset_path)\n",
    "mrc_validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# korquad 데이터셋이랑 형식 똑같이 만들어주기\n",
    "id_list0 = []\n",
    "title_list0 = []\n",
    "context_list0 = []\n",
    "question_list0 = []\n",
    "answers_list0 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, row in pd.DataFrame(mrc_validation_dataset).iterrows():\n",
    "    id_list0.append(row['id'])\n",
    "    title_list0.append(str(row['title']))\n",
    "    context_list0.append(str(row['context']))\n",
    "    question_list0.append(str(row['question']))\n",
    "    answers_list0.append(row['answers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrc_validation_dataset = {\n",
    "    \"id\" : id_list0,\n",
    "    \"title\" : title_list0,\n",
    "    \"context\" : context_list0,\n",
    "    \"question\" : question_list0,\n",
    "    \"answers\" : answers_list0,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 240\n",
       "})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "mrc_validation_dataset= Dataset.from_dict(mrc_validation_dataset)\n",
    "mrc_validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples['question'],\n",
    "        examples['context'],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 240/240 [00:00<00:00, 846.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = mrc_validation_dataset.map(\n",
    "            prepare_validation_features,\n",
    "            batched=True,\n",
    "            num_proc=preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering Class 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_data_collator: 여러개 example들을 collator해주는 역할,\n",
    "# TrainingArguments : 한번에 training arguments들을 합쳐서 주는..!\n",
    "from transformers import default_data_collator, TrainingArguments, EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Team All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Question-Answering task와 관련된 'Trainer'의 subclass 코드 입니다.\n",
    "\"\"\"\n",
    "\n",
    "from transformers import Trainer, is_datasets_available, is_torch_tpu_available\n",
    "from transformers.trainer_utils import PredictionOutput\n",
    "\n",
    "if is_datasets_available():\n",
    "    import datasets\n",
    "\n",
    "# Huggingface의 Trainer를 상속받아 QuestionAnswering을 위한 Trainer를 생성합니다.\n",
    "class QuestionAnsweringTrainer(Trainer):\n",
    "    def __init__(self, *args, eval_examples=None, post_process_function=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.eval_examples = eval_examples\n",
    "        self.post_process_function = post_process_function\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None):\n",
    "        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        eval_examples = self.eval_examples if eval_examples is None else eval_examples\n",
    "\n",
    "        # 일시적으로 metric computation를 불가능하게 한 상태이며, 해당 코드에서는 loop 내에서 metric 계산을 수행합니다.\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        try:\n",
    "            output = self.prediction_loop(\n",
    "                eval_dataloader,\n",
    "                description=\"Evaluation\",\n",
    "                # metric이 없으면 예측값을 모으는 이유가 없으므로 아래의 코드를 따르게 됩니다.\n",
    "                # self.args.prediction_loss_only\n",
    "                prediction_loss_only=True if compute_metrics is None else None,\n",
    "                ignore_keys=ignore_keys,\n",
    "            )\n",
    "        finally:\n",
    "            self.compute_metrics = compute_metrics\n",
    "\n",
    "        if isinstance(eval_dataset, datasets.Dataset):\n",
    "            eval_dataset.set_format(\n",
    "                type=eval_dataset.format[\"type\"],\n",
    "                columns=list(eval_dataset.features.keys()),\n",
    "            )\n",
    "\n",
    "        if self.post_process_function is not None and self.compute_metrics is not None:\n",
    "            eval_preds = self.post_process_function(\n",
    "                eval_examples, eval_dataset, output.predictions, self.args\n",
    "            )\n",
    "            metrics = self.compute_metrics(eval_preds)\n",
    "\n",
    "            self.log(metrics)\n",
    "        else:\n",
    "            metrics = {}\n",
    "\n",
    "        self.control = self.callback_handler.on_evaluate(\n",
    "            self.args, self.state, self.control, metrics\n",
    "        )\n",
    "        return metrics\n",
    "\n",
    "    def predict(self, test_dataset, test_examples, ignore_keys=None):\n",
    "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
    "\n",
    "        # 일시적으로 metric computation를 불가능하게 한 상태이며, 해당 코드에서는 loop 내에서 metric 계산을 수행합니다.\n",
    "        # evaluate 함수와 동일하게 구성되어있습니다\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        try:\n",
    "            output = self.prediction_loop(\n",
    "                test_dataloader,\n",
    "                description=\"Evaluation\",\n",
    "                # metric이 없으면 예측값을 모으는 이유가 없으므로 아래의 코드를 따르게 됩니다.\n",
    "                # self.args.prediction_loss_only\n",
    "                prediction_loss_only=True if compute_metrics is None else None,\n",
    "                ignore_keys=ignore_keys,\n",
    "            )\n",
    "        finally:\n",
    "            self.compute_metrics = compute_metrics\n",
    "\n",
    "        if self.post_process_function is None or self.compute_metrics is None:\n",
    "            return output\n",
    "\n",
    "        if isinstance(test_dataset, datasets.Dataset):\n",
    "            test_dataset.set_format(\n",
    "                type=test_dataset.format[\"type\"],\n",
    "                columns=list(test_dataset.features.keys()),\n",
    "            )\n",
    "\n",
    "        predictions = self.post_process_function(\n",
    "            test_examples, test_dataset, output.predictions, self.args\n",
    "        )\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 후처리 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Team All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the 'License');\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an 'AS IS' BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Pre-processing\n",
    "Post-processing utilities for question answering.\n",
    "\"\"\"\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from arguments import DataTrainingArguments, ModelArguments\n",
    "from datasets import DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PreTrainedTokenizerFast, TrainingArguments, is_torch_available\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "#from utils.datetime_helper import get_seoul_datetime_str\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 2024):\n",
    "    \"\"\"\n",
    "    seed 고정하는 함수 (random, numpy, torch)\n",
    "\n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if is_torch_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def postprocess_qa_predictions(\n",
    "    examples,\n",
    "    features,\n",
    "    predictions: Tuple[np.ndarray, np.ndarray],\n",
    "    version_2_with_negative: bool = False,\n",
    "    n_best_size: int = 20,\n",
    "    max_answer_length: int = 30,\n",
    "    null_score_diff_threshold: float = 0.0,\n",
    "    output_dir: Optional[str] = None,\n",
    "    prefix: Optional[str] = None,\n",
    "    is_world_process_zero: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Post-processes : qa model의 prediction 값을 후처리하는 함수\n",
    "    모델은 start logit과 end logit을 반환하기 때문에, 이를 기반으로 original text로 변경하는 후처리가 필요함\n",
    "\n",
    "    Args:\n",
    "        examples: 전처리 되지 않은 데이터셋 (see the main script for more information).\n",
    "        features: 전처리가 진행된 데이터셋 (see the main script for more information).\n",
    "        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\n",
    "            모델의 예측값 :start logits과 the end logits을 나타내는 two arrays              첫번째 차원은 :obj:`features`의 element와 갯수가 맞아야함.\n",
    "        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            정답이 없는 데이터셋이 포함되어있는지 여부를 나타냄\n",
    "        n_best_size (:obj:`int`, `optional`, defaults to 20):\n",
    "            답변을 찾을 때 생성할 n-best prediction 총 개수\n",
    "        max_answer_length (:obj:`int`, `optional`, defaults to 30):\n",
    "            생성할 수 있는 답변의 최대 길이\n",
    "        null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):\n",
    "            null 답변을 선택하는 데 사용되는 threshold\n",
    "            : if the best answer has a score that is less than the score of\n",
    "            the null answer minus this threshold, the null answer is selected for this example (note that the score of\n",
    "            the null answer for an example giving several features is the minimum of the scores for the null answer on\n",
    "            each feature: all features must be aligned on the fact they `want` to predict a null answer).\n",
    "            Only useful when :obj:`version_2_with_negative` is :obj:`True`.\n",
    "        output_dir (:obj:`str`, `optional`):\n",
    "            아래의 값이 저장되는 경로\n",
    "            dictionary : predictions, n_best predictions (with their scores and logits) if:obj:`version_2_with_negative=True`,\n",
    "            dictionary : the scores differences between best and null answers\n",
    "        prefix (:obj:`str`, `optional`):\n",
    "            dictionary에 `prefix`가 포함되어 저장됨\n",
    "        is_world_process_zero (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            이 프로세스가 main process인지 여부(logging/save를 수행해야 하는지 여부를 결정하는 데 사용됨)\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        len(predictions) == 2\n",
    "    ), \"`predictions` should be a tuple with two elements (start_logits, end_logits).\"\n",
    "    all_start_logits, all_end_logits = predictions\n",
    "\n",
    "    assert len(predictions[0]) == len(\n",
    "        features\n",
    "    ), f\"Got {len(predictions[0])} predictions and {len(features)} features.\"\n",
    "\n",
    "    # example과 mapping되는 feature 생성\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # prediction, nbest에 해당하는 OrderedDict 생성합니다.\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "    if version_2_with_negative:\n",
    "        scores_diff_json = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    logger.setLevel(logging.INFO if is_world_process_zero else logging.WARN)\n",
    "    logger.info(\n",
    "        f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\"\n",
    "    )\n",
    "\n",
    "    # 전체 example들에 대한 main Loop\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # 해당하는 현재 example index\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_prediction = None\n",
    "        prelim_predictions = []\n",
    "\n",
    "        # 현재 example에 대한 모든 feature 생성합니다.\n",
    "        for feature_index in feature_indices:\n",
    "            # 각 featureure에 대한 모든 prediction을 가져옵니다.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # logit과 original context의 logit을 mapping합니다.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "            # Optional : `token_is_max_context`, 제공되는 경우 현재 기능에서 사용할 수 있는 max context가 없는 answer를 제거합니다\n",
    "            token_is_max_context = features[feature_index].get(\n",
    "                \"token_is_max_context\", None\n",
    "            )\n",
    "\n",
    "            # minimum null prediction을 업데이트 합니다.\n",
    "            feature_null_score = start_logits[0] + end_logits[0]\n",
    "            if (\n",
    "                min_null_prediction is None\n",
    "                or min_null_prediction[\"score\"] > feature_null_score\n",
    "            ):\n",
    "                min_null_prediction = {\n",
    "                    \"offsets\": (0, 0),\n",
    "                    \"score\": feature_null_score,\n",
    "                    \"start_logit\": start_logits[0],\n",
    "                    \"end_logit\": end_logits[0],\n",
    "                }\n",
    "\n",
    "            # `n_best_size`보다 큰 start and end logits을 살펴봅니다.\n",
    "            start_indexes = np.argsort(start_logits)[\n",
    "                -1 : -n_best_size - 1 : -1\n",
    "            ].tolist()\n",
    "\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # out-of-scope answers는 고려하지 않습니다.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # 길이가 < 0 또는 > max_answer_length인 answer도 고려하지 않습니다.\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "                    # 최대 context가 없는 answer도 고려하지 않습니다.\n",
    "                    if (\n",
    "                        token_is_max_context is not None\n",
    "                        and not token_is_max_context.get(str(start_index), False)\n",
    "                    ):\n",
    "                        continue\n",
    "                    prelim_predictions.append(\n",
    "                        {\n",
    "                            \"offsets\": (\n",
    "                                offset_mapping[start_index][0],\n",
    "                                offset_mapping[end_index][1],\n",
    "                            ),\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"start_logit\": start_logits[start_index],\n",
    "                            \"end_logit\": end_logits[end_index],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if version_2_with_negative:\n",
    "            # minimum null prediction을 추가합니다.\n",
    "            prelim_predictions.append(min_null_prediction)\n",
    "            null_score = min_null_prediction[\"score\"]\n",
    "\n",
    "        # 가장 좋은 `n_best_size` predictions만 유지합니다.\n",
    "        predictions = sorted(\n",
    "            prelim_predictions, key=lambda x: x[\"score\"], reverse=True\n",
    "        )[:n_best_size]\n",
    "\n",
    "        # 낮은 점수로 인해 제거된 경우 minimum null prediction을 다시 추가합니다.\n",
    "        if version_2_with_negative and not any(\n",
    "            p[\"offsets\"] == (0, 0) for p in predictions\n",
    "        ):\n",
    "            predictions.append(min_null_prediction)\n",
    "\n",
    "        # offset을 사용하여 original context에서 answer text를 수집합니다.\n",
    "        context = example[\"context\"]\n",
    "        for pred in predictions:\n",
    "            offsets = pred.pop(\"offsets\")\n",
    "            pred[\"text\"] = context[offsets[0] : offsets[1]]\n",
    "\n",
    "        # rare edge case에는 null이 아닌 예측이 하나도 없으며 failure를 피하기 위해 fake prediction을 만듭니다.\n",
    "        if len(predictions) == 0 or (\n",
    "            len(predictions) == 1 and predictions[0][\"text\"] == \"\"\n",
    "        ):\n",
    "\n",
    "            predictions.insert(\n",
    "                0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0}\n",
    "            )\n",
    "\n",
    "        # 모든 점수의 소프트맥스를 계산합니다(we do it with numpy to stay independent from torch/tf in this file, using the LogSumExp trick).\n",
    "        scores = np.array([pred.pop(\"score\") for pred in predictions])\n",
    "        exp_scores = np.exp(scores - np.max(scores))\n",
    "        probs = exp_scores / exp_scores.sum()\n",
    "\n",
    "        # 예측값에 확률을 포함합니다.\n",
    "        for prob, pred in zip(probs, predictions):\n",
    "            pred[\"probability\"] = prob\n",
    "\n",
    "        # best prediction을 선택합니다.\n",
    "        if not version_2_with_negative:\n",
    "            all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n",
    "        else:\n",
    "            # else case : 먼저 비어 있지 않은 최상의 예측을 찾아야 합니다\n",
    "            i = 0\n",
    "            while predictions[i][\"text\"] == \"\":\n",
    "                i += 1\n",
    "            best_non_null_pred = predictions[i]\n",
    "\n",
    "            # threshold를 사용해서 null prediction을 비교합니다.\n",
    "            score_diff = (\n",
    "                null_score\n",
    "                - best_non_null_pred[\"start_logit\"]\n",
    "                - best_non_null_pred[\"end_logit\"]\n",
    "            )\n",
    "            scores_diff_json[example[\"id\"]] = float(score_diff)  # JSON-serializable 가능\n",
    "            if score_diff > null_score_diff_threshold:\n",
    "                all_predictions[example[\"id\"]] = \"\"\n",
    "            else:\n",
    "                all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n",
    "\n",
    "        # np.float를 다시 float로 casting -> `predictions`은 JSON-serializable 가능\n",
    "        all_nbest_json[example[\"id\"]] = [\n",
    "            {\n",
    "                k: (\n",
    "                    float(v)\n",
    "                    if isinstance(v, (np.float16, np.float32, np.float64))\n",
    "                    else v\n",
    "                )\n",
    "                for k, v in pred.items()\n",
    "            }\n",
    "            for pred in predictions\n",
    "        ]\n",
    "\n",
    "    # output_dir이 있으면 모든 dicts를 저장합니다.\n",
    "    if output_dir is not None:\n",
    "        assert os.path.isdir(output_dir), f\"{output_dir} is not a directory.\"\n",
    "\n",
    "        prediction_file = os.path.join(\n",
    "            output_dir,\n",
    "            \"predictions.json\" if prefix is None else f\"predictions_{prefix}.json\",\n",
    "        )\n",
    "        nbest_file = os.path.join(\n",
    "            output_dir,\n",
    "            \"nbest_predictions.json\"\n",
    "            if prefix is None\n",
    "            else f\"nbest_predictions_{prefix}.json\",\n",
    "        )\n",
    "        if version_2_with_negative:\n",
    "            null_odds_file = os.path.join(\n",
    "                output_dir,\n",
    "                \"null_odds.json\" if prefix is None else f\"null_odds_{prefix}.json\",\n",
    "            )\n",
    "\n",
    "        logger.info(f\"Saving predictions to {prediction_file}.\")\n",
    "        with open(prediction_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            writer.write(\n",
    "                json.dumps(all_predictions, indent=4, ensure_ascii=False) + \"\\n\"\n",
    "            )\n",
    "        logger.info(f\"Saving nbest_preds to {nbest_file}.\")\n",
    "        with open(nbest_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            writer.write(\n",
    "                json.dumps(all_nbest_json, indent=4, ensure_ascii=False) + \"\\n\"\n",
    "            )\n",
    "        if version_2_with_negative:\n",
    "            logger.info(f\"Saving null_odds to {null_odds_file}.\")\n",
    "            with open(null_odds_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "                writer.write(\n",
    "                    json.dumps(scores_diff_json, indent=4, ensure_ascii=False) + \"\\n\"\n",
    "                )\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "def check_no_error(\n",
    "    data_args: DataTrainingArguments,\n",
    "    training_args: TrainingArguments,\n",
    "    datasets: DatasetDict,\n",
    "    tokenizer,\n",
    ") -> Tuple[Any, int]:\n",
    "\n",
    "    # last checkpoint 찾기.\n",
    "    last_checkpoint = None\n",
    "    if (\n",
    "        os.path.isdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "    # Tokenizer check: 해당 script는 Fast tokenizer를 필요로합니다.\n",
    "    if not isinstance(tokenizer, PreTrainedTokenizerFast):\n",
    "        raise ValueError(\n",
    "            \"This example script only works for models that have a fast tokenizer. Checkout the big table of models \"\n",
    "            \"at https://huggingface.co/transformers/index.html#bigtable to find the model types that meet this \"\n",
    "            \"requirement\"\n",
    "        )\n",
    "\n",
    "    if data_args.max_seq_length > tokenizer.model_max_length:\n",
    "        logger.warn(\n",
    "            f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "    if \"validation\" not in datasets:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    return last_checkpoint, max_seq_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 후처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 이해하는 형태에서 사람이 이해하는 형태로 답변 매칭\n",
    "def post_processing_function(examples, features, predictions):\n",
    "    # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "    predictions = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        version_2_with_negative=False,\n",
    "        n_best_size=n_best_size,\n",
    "        max_answer_length=max_answer_length,\n",
    "        null_score_diff_threshold=0.0,\n",
    "        output_dir=training_args.output_dir,\n",
    "        is_world_process_zero=trainer.is_world_process_zero(),\n",
    "    )\n",
    "\n",
    "    # Format the result to the format the metric expects.\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "    return EvalPrediction(predictions=formatted_predictions, label_ids=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 2차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/level2-mrc-nlp-15/src/wandb/run-20241015_184122-0y11f1su</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp15/odqa/runs/0y11f1su' target=\"_blank\">electric-serenity-822</a></strong> to <a href='https://wandb.ai/nlp15/odqa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp15/odqa' target=\"_blank\">https://wandb.ai/nlp15/odqa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp15/odqa/runs/0y11f1su' target=\"_blank\">https://wandb.ai/nlp15/odqa/runs/0y11f1su</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp15/odqa/runs/0y11f1su?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f9c9bc4e3b0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project='odqa', # 실험기록을 관리한 프로젝트 이름\n",
    "           entity='nlp15', # 사용자명 또는 팀 이름        \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_total_limit=1,  # 저장할 체크포인트의 최대 수\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    #eval_steps=500,  # 몇 스텝마다 평가할지 설정\n",
    "    #logging_steps=500,  # 몇 스텝마다 로깅할지 설정,\n",
    "    #load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        eval_examples=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator, # 보통 default\n",
    "        post_process_function=post_processing_function, # function을 input으로 받음!\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5769\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1444\n",
      "  Number of trainable parameters = 335608834\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1444' max='1444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1444/1444 35:59, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.790100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.258400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs/checkpoint-500\n",
      "Configuration saved in outputs/checkpoint-500/config.json\n",
      "Model weights saved in outputs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to outputs/checkpoint-1000\n",
      "Configuration saved in outputs/checkpoint-1000/config.json\n",
      "Model weights saved in outputs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▅█</td></tr><tr><td>train/global_step</td><td>▁▅█</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>1444</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.2584</td></tr><tr><td>train/total_flos</td><td>2.143084255034573e+16</td></tr><tr><td>train/train_loss</td><td>0.38528</td></tr><tr><td>train/train_runtime</td><td>2161.033</td></tr><tr><td>train/train_samples_per_second</td><td>10.678</td></tr><tr><td>train/train_steps_per_second</td><td>0.668</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">electric-serenity-822</strong> at: <a href='https://wandb.ai/nlp15/odqa/runs/0y11f1su' target=\"_blank\">https://wandb.ai/nlp15/odqa/runs/0y11f1su</a><br/> View project at: <a href='https://wandb.ai/nlp15/odqa' target=\"_blank\">https://wandb.ai/nlp15/odqa</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241015_184122-0y11f1su/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 허깅페이스에 모델 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: sudo: command not found\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/tmpvxgja89o/config.json\n",
      "Model weights saved in /tmp/tmpvxgja89o/pytorch_model.bin\n",
      "Uploading the following files to ssunbear/klue_roberta_large_finetuned_korquad_v2: pytorch_model.bin,config.json\n",
      "pytorch_model.bin: 100%|██████████| 1.34G/1.34G [00:56<00:00, 23.7MB/s]   \n",
      "tokenizer config file saved in /tmp/tmp1q6nsung/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmp1q6nsung/special_tokens_map.json\n",
      "Uploading the following files to ssunbear/klue_roberta_large_finetuned_korquad_v2: tokenizer_config.json,vocab.txt,tokenizer.json,special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ssunbear/klue_roberta_large_finetuned_korquad_v2/commit/32b49af3b113769f65b0ab2392cfb81d4c159962', commit_message='Upload tokenizer', commit_description='', oid='32b49af3b113769f65b0ab2392cfb81d4c159962', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ssunbear/klue_roberta_large_finetuned_korquad_v2', endpoint='https://huggingface.co', repo_type='model', repo_id='ssunbear/klue_roberta_large_finetuned_korquad_v2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Huggingface Access Token\n",
    "ACCESS_TOKEN = #토큰아이디 입력하시면 됩니다.\n",
    "\n",
    "# Upload to Huggingface\n",
    "model.push_to_hub('klue_roberta_large_finetuned_korquad_v2', use_temp_dir=True, use_auth_token=ACCESS_TOKEN)\n",
    "tokenizer.push_to_hub('klue_roberta_large_finetuned_korquad_v2', use_temp_dir=True, use_auth_token=ACCESS_TOKEN)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "\n",
    "from datasets import DatasetDict\n",
    "import evaluate\n",
    "import argparse\n",
    "from trainer_qa import QuestionAnsweringTrainer\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from utils_qa import set_seed, check_no_error, postprocess_qa_predictions\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf import DictConfig\n",
    "from utils.naming import wandb_naming\n",
    "from prepare_dataset import prepare_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINISH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from CNN_layer_model import CNN_RobertaForQuestionAnswering\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "model_name = \"CurtisJeon/klue-roberta-large-korquad_v1_qa\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True\n",
    ")\n",
    "model = CNN_RobertaForQuestionAnswering.from_pretrained(\n",
    "    model_name,\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from custom_model_copy import CNN_RobertaForQuestionAnswering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of CNN_RobertaForQuestionAnswering were not initialized from the model checkpoint at CurtisJeon/klue-roberta-large-korquad_v1_qa and are newly initialized: ['cnn_block2.layer_norm.bias', 'cnn_block4.conv2.bias', 'cnn_block5.conv2.weight', 'cnn_block2.conv1.weight', 'cnn_block2.conv1.bias', 'cnn_block4.conv2.weight', 'cnn_block4.layer_norm.bias', 'cnn_block1.layer_norm.bias', 'cnn_block2.layer_norm.weight', 'cnn_block4.layer_norm.weight', 'cnn_block4.conv1.weight', 'cnn_block5.layer_norm.bias', 'cnn_block3.layer_norm.bias', 'cnn_block4.conv1.bias', 'cnn_block1.conv2.weight', 'cnn_block1.layer_norm.weight', 'cnn_block5.layer_norm.weight', 'cnn_block3.conv2.weight', 'cnn_block3.conv2.bias', 'cnn_block5.conv1.bias', 'cnn_block3.conv1.weight', 'cnn_block2.conv2.bias', 'cnn_block3.layer_norm.weight', 'cnn_block1.conv1.weight', 'cnn_block1.conv1.bias', 'cnn_block2.conv2.weight', 'cnn_block5.conv1.weight', 'cnn_block3.conv1.bias', 'cnn_block1.conv2.bias', 'cnn_block5.conv2.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"CurtisJeon/klue-roberta-large-korquad_v1_qa\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True\n",
    ")\n",
    "model = CNN_RobertaForQuestionAnswering.from_pretrained(\n",
    "    model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cnn_block1): CNN_block(\n",
       "    (conv1): Conv1d(514, 1028, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1028, 514, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (cnn_block2): CNN_block(\n",
       "    (conv1): Conv1d(514, 1028, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1028, 514, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (cnn_block3): CNN_block(\n",
       "    (conv1): Conv1d(514, 1028, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1028, 514, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (cnn_block4): CNN_block(\n",
       "    (conv1): Conv1d(514, 1028, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1028, 514, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (cnn_block5): CNN_block(\n",
       "    (conv1): Conv1d(514, 1028, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1028, 514, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
