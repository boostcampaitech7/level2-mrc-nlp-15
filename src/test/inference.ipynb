{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from typing import Callable, Dict, List, NoReturn, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from arguments import DataTrainingArguments, ModelArguments\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    load_from_disk,\n",
    "    load_metric,\n",
    ")\n",
    "from retrieval import BM25Retrieval\n",
    "from trainer_qa import QuestionAnsweringTrainer\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from utils_qa import check_no_error, postprocess_qa_predictions\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import os\n",
    "os.environ[\"DATA_PATH\"] = \"./data\"\n",
    "os.environ[\"CONTEXT_PATH\"] = \"wikipedia_documents.json\"\n",
    "\n",
    "def main():\n",
    "    # 가능한 arguments들은 ./arguments.py나 transformer package 안의 src/transformers/training_args.py에서 확인 가능합니다.\n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "    )\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    training_args.do_train = True\n",
    "\n",
    "    print(f\"model is from {model_args.model_name_or_path}\")\n",
    "    print(f\"data is from {data_args.dataset_name}\")\n",
    "\n",
    "    # logging 설정\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "    # verbosity 설정 : Transformers logger의 정보로 사용합니다 (on main process only)\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # 모델을 초기화하기 전에 난수를 고정합니다.\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    datasets = load_from_disk(data_args.dataset_name)\n",
    "    print(datasets)\n",
    "\n",
    "    # AutoConfig를 이용하여 pretrained model과 tokenizer를 불러옵니다.\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # True일 경우 : run passage retrieval\n",
    "    if data_args.eval_retrieval:\n",
    "        datasets = run_sparse_retrieval(\n",
    "            tokenizer.tokenize,\n",
    "            datasets,\n",
    "            training_args,\n",
    "            data_args,\n",
    "            data_path=os.environ.get(\"DATA_PATH\", \"../data/\"),\n",
    "            context_path=os.environ.get(\"CONTEXT_PATH\", \"wikipedia_documents.json\")\n",
    "        )\n",
    "\n",
    "    # eval or predict mrc model\n",
    "    if training_args.do_eval or training_args.do_predict:\n",
    "        run_mrc(data_args, training_args, model_args, datasets, tokenizer, model)\n",
    "\n",
    "def run_sparse_retrieval(\n",
    "    tokenize_fn: Callable[[str], List[str]],\n",
    "    datasets: DatasetDict,\n",
    "    training_args: TrainingArguments,\n",
    "    data_args: DataTrainingArguments,\n",
    "    data_path: str = \"../data\",\n",
    "    context_path: str = \"wikipedia_documents.json\",\n",
    ") -> DatasetDict:\n",
    "\n",
    "    # Query에 맞는 Passage들을 Retrieval 합니다.\n",
    "    retriever = BM25Retrieval(\n",
    "        tokenize_fn=tokenize_fn, data_path=data_path, context_path=context_path\n",
    "    )\n",
    "\n",
    "    df = retriever.retrieve(\n",
    "        datasets[\"validation\"], topk=data_args.top_k_retrieval\n",
    "    )\n",
    "\n",
    "    # test data에 대해선 정답이 없으므로 id question context로만 데이터셋이 구성됩니다.\n",
    "    if training_args.do_predict:\n",
    "        f = Features({\n",
    "            \"context\": Value(dtype=\"string\", id=None),\n",
    "            \"id\": Value(dtype=\"string\", id=None),\n",
    "            \"question\": Value(dtype=\"string\", id=None),\n",
    "        })\n",
    "\n",
    "    # train data에 대해선 정답이 존재하므로 id question context answer로 데이터셋이 구성됩니다.\n",
    "    elif training_args.do_eval:\n",
    "        f = Features({\n",
    "            \"answers\": Sequence(\n",
    "                feature={\"text\": Value(dtype=\"string\", id=None),\n",
    "                         \"answer_start\": Value(dtype=\"int32\", id=None)},\n",
    "                length=-1,\n",
    "                id=None,\n",
    "            ),\n",
    "            \"context\": Value(dtype=\"string\", id=None),\n",
    "            \"id\": Value(dtype=\"string\", id=None),\n",
    "            \"question\": Value(dtype=\"string\", id=None),\n",
    "        })\n",
    "    datasets = DatasetDict({\"validation\": Dataset.from_pandas(df, features=f)})\n",
    "    return datasets\n",
    "\n",
    "def run_mrc(\n",
    "    data_args: DataTrainingArguments,\n",
    "    training_args: TrainingArguments,\n",
    "    model_args: ModelArguments,\n",
    "    datasets: DatasetDict,\n",
    "    tokenizer,\n",
    "    model,\n",
    ") -> NoReturn:\n",
    "\n",
    "    # eval 혹은 prediction에서만 사용함\n",
    "    column_names = datasets[\"validation\"].column_names\n",
    "\n",
    "    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
    "\n",
    "    # Padding에 대한 옵션을 설정합니다.\n",
    "    # (question|context) 혹은 (context|question)로 세팅 가능합니다.\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "    # 오류가 있는지 확인합니다.\n",
    "    last_checkpoint, max_seq_length = check_no_error(\n",
    "        data_args, training_args, datasets, tokenizer\n",
    "    )\n",
    "\n",
    "    # Validation preprocessing / 전처리를 진행합니다.\n",
    "    def prepare_validation_features(examples):\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples[context_column_name if pad_on_right else question_column_name],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_seq_length,\n",
    "            stride=data_args.doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\" if data_args.pad_to_max_length else False,\n",
    "        )\n",
    "\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            context_index = 1 if pad_on_right else 0\n",
    "\n",
    "            sample_index = sample_mapping[i]\n",
    "            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "            tokenized_examples[\"offset_mapping\"][i] = [\n",
    "                (o if sequence_ids[k] == context_index else None)\n",
    "                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "            ]\n",
    "        return tokenized_examples\n",
    "\n",
    "    eval_dataset = datasets[\"validation\"]\n",
    "\n",
    "    # Validation Feature 생성\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        prepare_validation_features,\n",
    "        batched=True,\n",
    "        num_proc=data_args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None\n",
    "    )\n",
    "\n",
    "    # Post-processing:\n",
    "    def post_processing_function(examples, features, predictions, training_args):\n",
    "        predictions = postprocess_qa_predictions(\n",
    "            examples=examples,\n",
    "            features=features,\n",
    "            predictions=predictions,\n",
    "            max_answer_length=data_args.max_answer_length,\n",
    "            output_dir=training_args.output_dir,\n",
    "        )\n",
    "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "        \n",
    "        if training_args.do_predict:\n",
    "            return formatted_predictions\n",
    "        elif training_args.do_eval:\n",
    "            references = [\n",
    "                {\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]}\n",
    "                for ex in datasets[\"validation\"]\n",
    "            ]\n",
    "            return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n",
    "\n",
    "    metric = load_metric(\"squad\")\n",
    "\n",
    "    def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "        return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
    "\n",
    "    print(\"init trainer...\")\n",
    "    # Trainer 초기화\n",
    "    trainer = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=None,\n",
    "        eval_dataset=eval_dataset,\n",
    "        eval_examples=datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        post_process_function=post_processing_function,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    if training_args.do_predict:\n",
    "        predictions = trainer.predict(test_dataset=eval_dataset, test_examples=datasets[\"validation\"])\n",
    "        print(\"No metric can be presented because there is no correct answer given. Job done!\")\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        metrics = trainer.evaluate()\n",
    "        metrics[\"eval_samples\"] = len(eval_dataset)\n",
    "\n",
    "        trainer.log_metrics(\"test\", metrics)\n",
    "        trainer.save_metrics(\"test\", metrics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
