{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import NoReturn\n",
    "\n",
    "from arguments import DataTrainingArguments, ModelArguments\n",
    "from datasets import DatasetDict, load_from_disk, load_metric\n",
    "from trainer_qa import QuestionAnsweringTrainer\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from utils_qa import check_no_error, postprocess_qa_predictions\n",
    "\n",
    "\n",
    "seed = 2024\n",
    "deterministic = False\n",
    "\n",
    "random.seed(seed) # python random seed 고정\n",
    "np.random.seed(seed) # numpy random seed 고정\n",
    "torch.manual_seed(seed) # torch random seed 고정\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "if deterministic: # cudnn random seed 고정 - 고정 시 학습 속도가 느려질 수 있습니다. \n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 가능한 arguments 들은 ./arguments.py 나 transformer package 안의 src/transformers/training_args.py 에서 확인 가능합니다.\n",
    "    # --help flag 를 실행시켜서 확인할 수 도 있습니다.\n",
    "\n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "    )\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    print(model_args.model_name_or_path)\n",
    "\n",
    "    # [참고] argument를 manual하게 수정하고 싶은 경우에 아래와 같은 방식을 사용할 수 있습니다\n",
    "    # training_args.per_device_train_batch_size = 4\n",
    "    # print(training_args.per_device_train_batch_size)\n",
    "\n",
    "    print(f\"model is from {model_args.model_name_or_path}\")\n",
    "    print(f\"data is from {data_args.dataset_name}\")\n",
    "\n",
    "    # logging 설정\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -    %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "    # verbosity 설정 : Transformers logger의 정보로 사용합니다 (on main process only)\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # 모델을 초기화하기 전에 난수를 고정합니다.\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    datasets = load_from_disk(data_args.dataset_name)\n",
    "    print(datasets)\n",
    "\n",
    "    # AutoConfig를 이용하여 pretrained model 과 tokenizer를 불러옵니다.\n",
    "    # argument로 원하는 모델 이름을 설정하면 옵션을 바꿀 수 있습니다.\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name\n",
    "        if model_args.config_name is not None\n",
    "        else model_args.model_name_or_path,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name\n",
    "        if model_args.tokenizer_name is not None\n",
    "        else model_args.model_name_or_path,\n",
    "        # 'use_fast' argument를 True로 설정할 경우 rust로 구현된 tokenizer를 사용할 수 있습니다.\n",
    "        # False로 설정할 경우 python으로 구현된 tokenizer를 사용할 수 있으며,\n",
    "        # rust version이 비교적 속도가 빠릅니다.\n",
    "        use_fast=True,\n",
    "    )\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        type(training_args),\n",
    "        type(model_args),\n",
    "        type(datasets),\n",
    "        type(tokenizer),\n",
    "        type(model),\n",
    "    )\n",
    "\n",
    "    # do_train mrc model 혹은 do_eval mrc model\n",
    "    if training_args.do_train or training_args.do_eval:\n",
    "        run_mrc(data_args, training_args, model_args, datasets, tokenizer, model)\n",
    "\n",
    "\n",
    "def run_mrc(\n",
    "    data_args: DataTrainingArguments,\n",
    "    training_args: TrainingArguments,\n",
    "    model_args: ModelArguments,\n",
    "    datasets: DatasetDict,\n",
    "    tokenizer,\n",
    "    model,\n",
    ") -> NoReturn:\n",
    "\n",
    "    # dataset을 전처리합니다.\n",
    "    # training과 evaluation에서 사용되는 전처리는 아주 조금 다른 형태를 가집니다.\n",
    "    if training_args.do_train:\n",
    "        column_names = datasets[\"train\"].column_names\n",
    "    else:\n",
    "        column_names = datasets[\"validation\"].column_names\n",
    "\n",
    "    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
    "\n",
    "    # Padding에 대한 옵션을 설정합니다.\n",
    "    # (question|context) 혹은 (context|question)로 세팅 가능합니다.\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "    # 오류가 있는지 확인합니다.\n",
    "    last_checkpoint, max_seq_length = check_no_error(\n",
    "        data_args, training_args, datasets, tokenizer\n",
    "    )\n",
    "\n",
    "    # Train preprocessing / 전처리를 진행합니다.\n",
    "    def prepare_train_features(examples):\n",
    "        # truncation과 padding(length가 짧을때만)을 통해 toknization을 진행하며, stride를 이용하여 overflow를 유지합니다.\n",
    "        # 각 example들은 이전의 context와 조금씩 겹치게됩니다.\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples[context_column_name if pad_on_right else question_column_name],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_seq_length,\n",
    "            stride=data_args.doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_token_type_ids=False, # roberta모델을 사용할 경우 False, bert를 사용할 경우 True로 표기해야합니다.\n",
    "            padding=\"max_length\" if data_args.pad_to_max_length else False,\n",
    "        )\n",
    "\n",
    "        # 길이가 긴 context가 등장할 경우 truncate를 진행해야하므로, 해당 데이터셋을 찾을 수 있도록 mapping 가능한 값이 필요합니다.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        # token의 캐릭터 단위 position를 찾을 수 있도록 offset mapping을 사용합니다.\n",
    "        # start_positions과 end_positions을 찾는데 도움을 줄 수 있습니다.\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "        # 데이터셋에 \"start position\", \"enc position\" label을 부여합니다.\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)  # cls index\n",
    "\n",
    "            # sequence id를 설정합니다 (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            # 하나의 example이 여러개의 span을 가질 수 있습니다.\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[answer_column_name][sample_index]\n",
    "\n",
    "            # answer가 없을 경우 cls_index를 answer로 설정합니다(== example에서 정답이 없는 경우 존재할 수 있음).\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # text에서 정답의 Start/end character index\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "                # text에서 current span의 Start token index\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "\n",
    "                # text에서 current span의 End token index\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                # 정답이 span을 벗어났는지 확인합니다(정답이 없는 경우 CLS index로 label되어있음).\n",
    "                if not (\n",
    "                    offsets[token_start_index][0] <= start_char\n",
    "                    and offsets[token_end_index][1] >= end_char\n",
    "                ):\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                else:\n",
    "                    # token_start_index 및 token_end_index를 answer의 끝으로 이동합니다.\n",
    "                    # Note: answer가 마지막 단어인 경우 last offset을 따라갈 수 있습니다(edge case).\n",
    "                    while (\n",
    "                        token_start_index < len(offsets)\n",
    "                        and offsets[token_start_index][0] <= start_char\n",
    "                    ):\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "        return tokenized_examples\n",
    "\n",
    "    if training_args.do_train:\n",
    "        if \"train\" not in datasets:\n",
    "            raise ValueError(\"--do_train requires a train dataset\")\n",
    "        train_dataset = datasets[\"train\"]\n",
    "\n",
    "        # dataset에서 train feature를 생성합니다.\n",
    "        train_dataset = train_dataset.map(\n",
    "            prepare_train_features,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "        )\n",
    "\n",
    "    # Validation preprocessing\n",
    "    def prepare_validation_features(examples):\n",
    "        # truncation과 padding(length가 짧을때만)을 통해 toknization을 진행하며, stride를 이용하여 overflow를 유지합니다.\n",
    "        # 각 example들은 이전의 context와 조금씩 겹치게됩니다.\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples[context_column_name if pad_on_right else question_column_name],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_seq_length,\n",
    "            stride=data_args.doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_token_type_ids=False, # roberta모델을 사용할 경우 False, bert를 사용할 경우 True로 표기해야합니다.\n",
    "            padding=\"max_length\" if data_args.pad_to_max_length else False,\n",
    "        )\n",
    "\n",
    "        # 길이가 긴 context가 등장할 경우 truncate를 진행해야하므로, 해당 데이터셋을 찾을 수 있도록 mapping 가능한 값이 필요합니다.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "        # evaluation을 위해, prediction을 context의 substring으로 변환해야합니다.\n",
    "        # corresponding example_id를 유지하고 offset mappings을 저장해야합니다.\n",
    "        tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "            # sequence id를 설정합니다 (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            context_index = 1 if pad_on_right else 0\n",
    "\n",
    "            # 하나의 example이 여러개의 span을 가질 수 있습니다.\n",
    "            sample_index = sample_mapping[i]\n",
    "            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "            # Set to None the offset_mapping을 None으로 설정해서 token position이 context의 일부인지 쉽게 판별 할 수 있습니다.\n",
    "            tokenized_examples[\"offset_mapping\"][i] = [\n",
    "                (o if sequence_ids[k] == context_index else None)\n",
    "                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "            ]\n",
    "        return tokenized_examples\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        eval_dataset = datasets[\"validation\"]\n",
    "\n",
    "        # Validation Feature 생성\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            prepare_validation_features,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "        )\n",
    "\n",
    "    # Data collator\n",
    "    # flag가 True이면 이미 max length로 padding된 상태입니다.\n",
    "    # 그렇지 않다면 data collator에서 padding을 진행해야합니다.\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None\n",
    "    )\n",
    "\n",
    "    # Post-processing:\n",
    "    def post_processing_function(examples, features, predictions, training_args):\n",
    "        # Post-processing: start logits과 end logits을 original context의 정답과 match시킵니다.\n",
    "        predictions = postprocess_qa_predictions(\n",
    "            examples=examples,\n",
    "            features=features,\n",
    "            predictions=predictions,\n",
    "            max_answer_length=data_args.max_answer_length,\n",
    "            output_dir=training_args.output_dir,\n",
    "        )\n",
    "        # Metric을 구할 수 있도록 Format을 맞춰줍니다.\n",
    "        formatted_predictions = [\n",
    "            {\"id\": k, \"prediction_text\": v} for k, v in predictions.items()\n",
    "        ]\n",
    "        if training_args.do_predict:\n",
    "            return formatted_predictions\n",
    "\n",
    "        elif training_args.do_eval:\n",
    "            references = [\n",
    "                {\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]}\n",
    "                for ex in datasets[\"validation\"]\n",
    "            ]\n",
    "            return EvalPrediction(\n",
    "                predictions=formatted_predictions, label_ids=references\n",
    "            )\n",
    "\n",
    "    metric = load_metric(\"squad\")\n",
    "\n",
    "    def compute_metrics(p: EvalPrediction):\n",
    "        return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
    "\n",
    "    # Trainer 초기화\n",
    "    trainer = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset if training_args.do_train else None,\n",
    "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "        eval_examples=datasets[\"validation\"] if training_args.do_eval else None,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        post_process_function=post_processing_function,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        if last_checkpoint is not None:\n",
    "            checkpoint = last_checkpoint\n",
    "        elif os.path.isdir(model_args.model_name_or_path):\n",
    "            checkpoint = model_args.model_name_or_path\n",
    "        else:\n",
    "            checkpoint = None\n",
    "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "        trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "        metrics = train_result.metrics\n",
    "        metrics[\"train_samples\"] = len(train_dataset)\n",
    "\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "\n",
    "        output_train_file = os.path.join(training_args.output_dir, \"train_results.txt\")\n",
    "\n",
    "        with open(output_train_file, \"w\") as writer:\n",
    "            logger.info(\"***** Train results *****\")\n",
    "            for key, value in sorted(train_result.metrics.items()):\n",
    "                logger.info(f\"  {key} = {value}\")\n",
    "                writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "        # State 저장\n",
    "        trainer.state.save_to_json(\n",
    "            os.path.join(training_args.output_dir, \"trainer_state.json\")\n",
    "        )\n",
    "\n",
    "    # Evaluation\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "        metrics = trainer.evaluate()\n",
    "\n",
    "        metrics[\"eval_samples\"] = len(eval_dataset)\n",
    "\n",
    "        trainer.log_metrics(\"eval\", metrics)\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
